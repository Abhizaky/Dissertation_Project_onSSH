{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4faca1c-0adb-43ef-802c-adde19a2fa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90467993-ee8b-4a32-9a59-fb194d138cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/ABHISHEK/OneDrive/Documents/SSH Data/ostst-single-layer-fd-lat-40-urms-5-kf-13-kr-4-beta.nc'\n",
    "\n",
    "ds = xr.open_dataset(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c443995-3539-42dc-b6de-842c8f5cc76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh = ds['ssh']  \n",
    "\n",
    "\n",
    "x_norm = (ds.x - ds.x.min()) / (ds.x.max() - ds.x.min())\n",
    "y_norm = (ds.y - ds.y.min()) / (ds.y.max() - ds.y.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "657e06ae-4a01-47ff-a0a7-a96eb92ede7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh_mean = ssh.mean()\n",
    "ssh_std = ssh.std()\n",
    "ssh_norm = (ssh - ssh_mean) / ssh_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6de2af9-81e8-40d6-beb0-138cd8ffa69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSH Original range: [-0.0719, 0.0775]\n",
      "SSH Normalized range: [-3.3165, 3.5742]\n",
      "SSH Mean: 0.0000, SSH Std: 0.0217\n"
     ]
    }
   ],
   "source": [
    "print(f\"SSH Original range: [{ssh.min().values:.4f}, {ssh.max().values:.4f}]\")\n",
    "print(f\"SSH Normalized range: [{ssh_norm.min().values:.4f}, {ssh_norm.max().values:.4f}]\")\n",
    "print(f\"SSH Mean: {ssh_mean.values:.4f}, SSH Std: {ssh_std.values:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "02b72736-4b3b-4227-b8d7-7da2ed944b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enhanced_ssh_dataset(ssh_norm, max_slices=100, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Create enhanced training dataset utilizing maximum available data\n",
    "    \"\"\"\n",
    "    total_available = len(ssh_norm.t)\n",
    "    print(f\"Total available time slices: {total_available}\")\n",
    "    \n",
    "    # Calculating max_slices\n",
    "    if total_available > max_slices:\n",
    "        step_size = max(1, total_available // max_slices)\n",
    "        time_indices = np.arange(0, total_available, step_size)[:max_slices]\n",
    "    else:\n",
    "        time_indices = np.arange(0, total_available)\n",
    "    \n",
    "    print(f\"Using {len(time_indices)} time slices (step size: {step_size if total_available > max_slices else 1})\")\n",
    "    \n",
    "    # Extract SSH slices\n",
    "    ssh_slices = ssh_norm.isel(t=time_indices).values\n",
    "    \n",
    "    # Split into training and validation\n",
    "    split_idx = int(train_ratio * len(ssh_slices))\n",
    "    \n",
    "    train_slices = ssh_slices[:split_idx]\n",
    "    val_slices = ssh_slices[split_idx:]\n",
    "    \n",
    "    print(f\"Training slices: {len(train_slices)}\")\n",
    "    print(f\"Validation slices: {len(val_slices)}\")\n",
    "    \n",
    "    return train_slices, val_slices, len(time_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "940d38bd-5d03-4409-8c27-c61c36a08c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total available time slices: 1826\n",
      "Using 100 time slices (step size: 18)\n",
      "Training slices: 80\n",
      "Validation slices: 20\n"
     ]
    }
   ],
   "source": [
    "ssh_train_enhanced, ssh_val_enhanced, total_slices_used = create_enhanced_ssh_dataset(ssh_norm, max_slices=100, train_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "93ffeed9-47bf-4bf6-9cb2-90a9ec56266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(ssh_slices, missing_rate=0.15):\n",
    "    \"\"\"Create training data with masks for multiple time slices\"\"\"\n",
    "    n_times, height, width = ssh_slices.shape\n",
    "    \n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in range(n_times):\n",
    "        ssh_slice = ssh_slices[i]\n",
    "        \n",
    "        \n",
    "        mask = np.ones_like(ssh_slice)\n",
    "        missing_pixels = np.random.rand(*ssh_slice.shape) < missing_rate\n",
    "        mask[missing_pixels] = 0\n",
    "        \n",
    "       \n",
    "        masked_ssh = ssh_slice * mask\n",
    "        \n",
    "        \n",
    "        input_tensor = np.stack([masked_ssh, mask], axis=-1)\n",
    "        target_tensor = np.stack([ssh_slice, mask], axis=-1)\n",
    "        \n",
    "        inputs.append(input_tensor)\n",
    "        targets.append(target_tensor)\n",
    "    \n",
    "    return np.array(inputs), np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a967f229-4daf-483c-bdd7-9fbc4546e357",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enhanced, y_train_enhanced = create_training_data(ssh_train_enhanced, missing_rate=0.15)\n",
    "X_val_enhanced, y_val_enhanced = create_training_data(ssh_val_enhanced, missing_rate=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f8570f87-ceac-489d-adb3-40c4ae008ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced training data shape: (80, 256, 256, 2)\n",
      "Enhanced validation data shape: (20, 256, 256, 2)\n",
      "Total data utilization: 5.5%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Enhanced training data shape: {X_train_enhanced.shape}\")\n",
    "print(f\"Enhanced validation data shape: {X_val_enhanced.shape}\")\n",
    "print(f\"Total data utilization: {total_slices_used/len(ssh_norm.t)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c25b2d1c-0caf-428f-8f10-a5ed63a9f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_enhanced_unet_2d(input_shape, dropout_rate=0.2):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    c1 = layers.Conv2D(32, (3,3), activation='relu', padding='same')(inputs)\n",
    "    c1 = layers.Dropout(dropout_rate)(c1)\n",
    "    c1 = layers.Conv2D(32, (3,3), activation='relu', padding='same')(c1)\n",
    "    p1 = layers.MaxPooling2D((2,2))(c1)\n",
    "    \n",
    "    c2 = layers.Conv2D(64, (3,3), activation='relu', padding='same')(p1)\n",
    "    c2 = layers.Dropout(dropout_rate)(c2)\n",
    "    c2 = layers.Conv2D(64, (3,3), activation='relu', padding='same')(c2)\n",
    "    p2 = layers.MaxPooling2D((2,2))(c2)\n",
    "    \n",
    "    c3 = layers.Conv2D(128, (3,3), activation='relu', padding='same')(p2)\n",
    "    c3 = layers.Dropout(dropout_rate)(c3)\n",
    "    c3 = layers.Conv2D(128, (3,3), activation='relu', padding='same')(c3)\n",
    "    p3 = layers.MaxPooling2D((2,2))(c3)\n",
    "    \n",
    "    # Bottleneck\n",
    "    c4 = layers.Conv2D(256, (3,3), activation='relu', padding='same')(p3)\n",
    "    c4 = layers.Dropout(dropout_rate)(c4)\n",
    "    c4 = layers.Conv2D(256, (3,3), activation='relu', padding='same')(c4)\n",
    "    \n",
    "    # Decoder\n",
    "    u1 = layers.UpSampling2D((2,2))(c4)\n",
    "    u1 = layers.Concatenate()([u1, c3])\n",
    "    c5 = layers.Conv2D(128, (3,3), activation='relu', padding='same')(u1)\n",
    "    c5 = layers.Dropout(dropout_rate)(c5)\n",
    "    c5 = layers.Conv2D(128, (3,3), activation='relu', padding='same')(c5)\n",
    "    \n",
    "    u2 = layers.UpSampling2D((2,2))(c5)\n",
    "    u2 = layers.Concatenate()([u2, c2])\n",
    "    c6 = layers.Conv2D(64, (3,3), activation='relu', padding='same')(u2)\n",
    "    c6 = layers.Dropout(dropout_rate)(c6)\n",
    "    c6 = layers.Conv2D(64, (3,3), activation='relu', padding='same')(c6)\n",
    "    \n",
    "    u3 = layers.UpSampling2D((2,2))(c6)\n",
    "    u3 = layers.Concatenate()([u3, c1])\n",
    "    c7 = layers.Conv2D(32, (3,3), activation='relu', padding='same')(u3)\n",
    "    c7 = layers.Dropout(dropout_rate)(c7)\n",
    "    c7 = layers.Conv2D(32, (3,3), activation='relu', padding='same')(c7)\n",
    "    \n",
    "    outputs = layers.Conv2D(1, (1,1), activation='linear')(c7)\n",
    "    \n",
    "    return models.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fb1d4bf6-0b0c-40ff-aa8f-ece49b429539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "def masked_mse(y_true, y_pred):\n",
    "    \"\"\"Masked Mean Squared Error loss\"\"\"\n",
    "    mask = y_true[..., 1:2]\n",
    "    target = y_true[..., :1]\n",
    "    diff = (target - y_pred) * mask\n",
    "    return K.sum(K.square(diff)) / (K.sum(mask) + 1e-8)\n",
    "\n",
    "def masked_mae(y_true, y_pred):\n",
    "    \"\"\"Masked Mean Absolute Error\"\"\"\n",
    "    mask = y_true[..., 1:2]\n",
    "    target = y_true[..., :1]\n",
    "    diff = K.abs(target - y_pred) * mask\n",
    "    return K.sum(diff) / (K.sum(mask) + 1e-8)\n",
    "\n",
    "def r2_metric(y_true, y_pred):\n",
    "    \"\"\"RÂ² score for masked data\"\"\"\n",
    "    mask = y_true[..., 1:2]\n",
    "    target = y_true[..., :1]\n",
    "    \n",
    "    y_pred_masked = y_pred * mask\n",
    "    y_true_masked = target * mask\n",
    "    \n",
    "    ss_res = K.sum(K.square(y_true_masked - y_pred_masked))\n",
    "    ss_tot = K.sum(K.square(y_true_masked - K.mean(y_true_masked)))\n",
    "    return 1 - (ss_res / (ss_tot + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "84a18afc-938c-4653-876e-cac829fcf39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train_enhanced.shape[1:]\n",
    "model_enhanced = build_enhanced_unet_2d(input_shape)\n",
    "model_enhanced.compile(\n",
    "    optimizer='adam',\n",
    "    loss=masked_mse,\n",
    "    metrics=[masked_mae, r2_metric]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0a10f801-e36f-4976-9925-7597bbbb2810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "early_stop_enhanced = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=30,  # Increased patience for larger dataset\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr_enhanced = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=15,  # Increased patience\n",
    "    min_lr=1e-8,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5771bbbf-db73-4cfe-adeb-ed2d4a8230ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with enhanced dataset...\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'model_3/conv2d_58/Relu' defined at (most recent call last):\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\ABHISHEK\\AppData\\Local\\Temp\\ipykernel_15824\\1064350715.py\", line 2, in <module>\n      history_enhanced = model_enhanced.fit(\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\layers\\convolutional\\base_conv.py\", line 314, in call\n      return self.activation(outputs)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\activations.py\", line 317, in relu\n      return backend.relu(\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\backend.py\", line 5366, in relu\n      x = tf.nn.relu(x)\nNode: 'model_3/conv2d_58/Relu'\nOOM when allocating tensor with shape[16,192,128,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model_3/conv2d_58/Relu}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_4731]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with enhanced dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m history_enhanced \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_enhanced\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_enhanced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_enhanced\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val_enhanced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_enhanced\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Increased epochs for larger dataset\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Increased batch size for efficiency\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stop_enhanced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr_enhanced\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'model_3/conv2d_58/Relu' defined at (most recent call last):\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\ABHISHEK\\AppData\\Local\\Temp\\ipykernel_15824\\1064350715.py\", line 2, in <module>\n      history_enhanced = model_enhanced.fit(\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\layers\\convolutional\\base_conv.py\", line 314, in call\n      return self.activation(outputs)\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\activations.py\", line 317, in relu\n      return backend.relu(\n    File \"C:\\Users\\ABHISHEK\\anaconda3\\envs\\ssh_prediction\\lib\\site-packages\\keras\\backend.py\", line 5366, in relu\n      x = tf.nn.relu(x)\nNode: 'model_3/conv2d_58/Relu'\nOOM when allocating tensor with shape[16,192,128,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model_3/conv2d_58/Relu}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_4731]"
     ]
    }
   ],
   "source": [
    "print(\"Training with enhanced dataset...\")\n",
    "history_enhanced = model_enhanced.fit(\n",
    "    X_train_enhanced, y_train_enhanced,\n",
    "    validation_data=(X_val_enhanced, y_val_enhanced),\n",
    "    epochs=150,  # Increased epochs for larger dataset\n",
    "    batch_size=16,  # Increased batch size for efficiency\n",
    "    callbacks=[early_stop_enhanced, reduce_lr_enhanced],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f356321-c145-42ab-80b6-111b8e6d3a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_slice_comparison(model, ssh_slices, ssh_mean, ssh_std, n_comparisons=4):\n",
    "    \"\"\"\n",
    "    Simple visualization of training slices vs true slices\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(3, n_comparisons, figsize=(16, 12))\n",
    "    \n",
    "    for i in range(n_comparisons):\n",
    "        # Create test data for this slice\n",
    "        X_test, y_test = create_training_data(ssh_slices[i:i+1], missing_rate=0.15)\n",
    "        \n",
    "        # Make prediction\n",
    "        pred = model.predict(X_test, verbose=0)[0, :, :, 0]\n",
    "        \n",
    "        # Extract components\n",
    "        original_slice = ssh_slices[i]\n",
    "        masked_input = X_test[0, :, :, 0]\n",
    "        mask = X_test[0, :, :, 1]\n",
    "        \n",
    "        # Denormalize for visualization\n",
    "        original_denorm = original_slice * ssh_std.values + ssh_mean.values\n",
    "        masked_denorm = masked_input * ssh_std.values + ssh_mean.values\n",
    "        pred_denorm = pred * ssh_std.values + ssh_mean.values\n",
    "        \n",
    "        # Set consistent color scale\n",
    "        vmin, vmax = original_denorm.min(), original_denorm.max()\n",
    "        \n",
    "        # Row 1: TRUE SSH\n",
    "        im1 = axes[0, i].imshow(original_denorm, cmap='RdYlBu_r', vmin=vmin, vmax=vmax)\n",
    "        axes[0, i].set_title(f'TRUE SSH - Slice {i+1}', fontweight='bold')\n",
    "        plt.colorbar(im1, ax=axes[0, i], fraction=0.046)\n",
    "        \n",
    "        # Row 2: MASKED INPUT (what model receives for training)\n",
    "        masked_display = np.where(mask == 1, masked_denorm, np.nan)\n",
    "        im2 = axes[1, i].imshow(masked_display, cmap='RdYlBu_r', vmin=vmin, vmax=vmax)\n",
    "        axes[1, i].set_title(f'TRAINING INPUT - Slice {i+1}\\n(15% missing)', fontweight='bold')\n",
    "        plt.colorbar(im2, ax=axes[1, i], fraction=0.046)\n",
    "        \n",
    "        # Row 3: MODEL PREDICTION\n",
    "        im3 = axes[2, i].imshow(pred_denorm, cmap='RdYlBu_r', vmin=vmin, vmax=vmax)\n",
    "        axes[2, i].set_title(f'PREDICTION - Slice {i+1}', fontweight='bold')\n",
    "        plt.colorbar(im3, ax=axes[2, i], fraction=0.046)\n",
    "        \n",
    "        # Calculate and print metrics\n",
    "        observed_mask = mask == 1\n",
    "        if np.sum(observed_mask) > 0:\n",
    "            r2 = r2_score(original_denorm[observed_mask], pred_denorm[observed_mask])\n",
    "            rmse = np.sqrt(mean_squared_error(original_denorm[observed_mask], pred_denorm[observed_mask]))\n",
    "            print(f\"Slice {i+1} - RÂ²: {r2:.4f}, RMSE: {rmse:.4f}m\")\n",
    "    \n",
    "    # Add row labels\n",
    "    axes[0, 0].set_ylabel('TRUE SSH', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('TRAINING INPUT', fontsize=12, fontweight='bold')\n",
    "    axes[2, 0].set_ylabel('PREDICTION', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('SSH Inpainting: True vs Training Slices Comparison', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7ec4d1-08f4-4fc6-9ee9-94a090ddf2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced analysis function (your existing function modified)\n",
    "def plot_essential_analysis(history, model, X_val, y_val, ssh_mean, ssh_std):\n",
    "    \"\"\"Create focused plots for essential model analysis\"\"\"\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_val)\n",
    "    \n",
    "    # Select first validation sample for detailed analysis\n",
    "    sample_idx = 0\n",
    "    true_ssh = y_val[sample_idx, :, :, 0]\n",
    "    pred_ssh = predictions[sample_idx, :, :, 0]\n",
    "    mask = y_val[sample_idx, :, :, 1]\n",
    "    \n",
    "    # Denormalize data\n",
    "    true_ssh_denorm = true_ssh * ssh_std.values + ssh_mean.values\n",
    "    pred_ssh_denorm = pred_ssh * ssh_std.values + ssh_mean.values\n",
    "    \n",
    "    # Only use observed points (where mask = 1)\n",
    "    observed_mask = mask == 1\n",
    "    true_obs = true_ssh_denorm[observed_mask]\n",
    "    pred_obs = pred_ssh_denorm[observed_mask]\n",
    "    \n",
    "    # Calculate comprehensive metrics\n",
    "    r2 = r2_score(true_obs, pred_obs)\n",
    "    rmse = np.sqrt(mean_squared_error(true_obs, pred_obs))\n",
    "    mae = mean_absolute_error(true_obs, pred_obs)\n",
    "    \n",
    "    # Calculate different accuracy thresholds\n",
    "    residuals = pred_obs - true_obs\n",
    "    accuracy_1pct = np.mean(np.abs(residuals) < 0.01 * np.abs(true_obs)) * 100\n",
    "    accuracy_5pct = np.mean(np.abs(residuals) < 0.05 * np.abs(true_obs)) * 100\n",
    "    accuracy_10pct = np.mean(np.abs(residuals) < 0.10 * np.abs(true_obs)) * 100\n",
    "    \n",
    "    # Correlation coefficient\n",
    "    correlation = np.corrcoef(true_obs, pred_obs)[0, 1]\n",
    "    \n",
    "    # Create subplot layout (2x2)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Loss Function Analysis (Overfitting Check)\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(history.history['loss'], label='Training Loss', linewidth=2.5, color='blue')\n",
    "    ax1.plot(history.history['val_loss'], label='Validation Loss', linewidth=2.5, color='red')\n",
    "    ax1.set_title('Loss Function - Overfitting Check', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Masked MSE Loss', fontsize=12)\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add overfitting assessment\n",
    "    final_train_loss = history.history['loss'][-1]\n",
    "    final_val_loss = history.history['val_loss'][-1]\n",
    "    overfitting_gap = final_val_loss - final_train_loss\n",
    "    \n",
    "    if overfitting_gap > 0.1:\n",
    "        overfitting_status = \"Potential Overfitting\"\n",
    "        color = 'red'\n",
    "    elif overfitting_gap > 0.05:\n",
    "        overfitting_status = \"Slight Overfitting\"\n",
    "        color = 'orange'\n",
    "    else:\n",
    "        overfitting_status = \"Good Generalization\"\n",
    "        color = 'green'\n",
    "    \n",
    "    ax1.text(0.02, 0.98, f'Status: {overfitting_status}\\nGap: {overfitting_gap:.4f}', \n",
    "             transform=ax1.transAxes, fontsize=10, verticalalignment='top',\n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=color, alpha=0.3))\n",
    "    \n",
    "    # 2. Learning Rate Schedule\n",
    "    ax2 = axes[0, 1]\n",
    "    if 'lr' in history.history:\n",
    "        ax2.plot(history.history['lr'], linewidth=2.5, color='green')\n",
    "        ax2.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('Epoch', fontsize=12)\n",
    "        ax2.set_ylabel('Learning Rate', fontsize=12)\n",
    "        ax2.set_yscale('log')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'Learning Rate\\nHistory Not Available', \n",
    "                transform=ax2.transAxes, ha='center', va='center', \n",
    "                fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"))\n",
    "        ax2.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 3. Prediction vs True SSH Scatter Plot\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.scatter(true_obs, pred_obs, alpha=0.6, s=15, c='blue', edgecolors='none')\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(true_obs.min(), pred_obs.min())\n",
    "    max_val = max(true_obs.max(), pred_obs.max())\n",
    "    ax3.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2.5, label='Perfect Prediction')\n",
    "    \n",
    "    ax3.set_xlabel('True SSH (m)', fontsize=12)\n",
    "    ax3.set_ylabel('Predicted SSH (m)', fontsize=12)\n",
    "    ax3.set_title('Prediction vs True SSH\\n(Observed Points Only)', fontsize=14, fontweight='bold')\n",
    "    ax3.legend(fontsize=11)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_aspect('equal', adjustable='box')\n",
    "    \n",
    "    # 4. Performance Metrics Display\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    # Final training metrics\n",
    "    final_train_r2 = history.history['r2_metric'][-1] if 'r2_metric' in history.history else 'N/A'\n",
    "    final_val_r2 = history.history['val_r2_metric'][-1] if 'val_r2_metric' in history.history else 'N/A'\n",
    "    \n",
    "    metrics_text = f\"\"\"\n",
    "    **PERFORMANCE METRICS**\n",
    "    \n",
    "    âââââââââââââââââââââââââââ\n",
    "    \n",
    "    **RÂ² Score:** {r2:.4f}\n",
    "    **RMSE:** {rmse:.4f} m\n",
    "    **MAE:** {mae:.4f} m\n",
    "    **Correlation:** {correlation:.4f}\n",
    "    \n",
    "    **Accuracy Metrics:**\n",
    "    â¢ Within Â±1%: {accuracy_1pct:.1f}%\n",
    "    â¢ Within Â±5%: {accuracy_5pct:.1f}%\n",
    "    â¢ Within Â±10%: {accuracy_10pct:.1f}%\n",
    "    \n",
    "    **Training Results:**\n",
    "    â¢ Final Train Loss: {final_train_loss:.4f}\n",
    "    â¢ Final Val Loss: {final_val_loss:.4f}\n",
    "    â¢ Final Train RÂ²: {final_train_r2:.4f}\n",
    "    â¢ Final Val RÂ²: {final_val_r2:.4f}\n",
    "    \n",
    "    **Data Info:**\n",
    "    â¢ Observed Points: {len(true_obs):,}\n",
    "    â¢ SSH Mean: {ssh_mean.values:.4f} m\n",
    "    â¢ SSH Std: {ssh_std.values:.4f} m\n",
    "    \"\"\"\n",
    "    \n",
    "    # Color-code the metrics based on performance\n",
    "    if r2 > 0.9:\n",
    "        bg_color = \"lightgreen\"\n",
    "    elif r2 > 0.8:\n",
    "        bg_color = \"lightyellow\"\n",
    "    else:\n",
    "        bg_color = \"lightcoral\"\n",
    "    \n",
    "    ax4.text(0.05, 0.95, metrics_text, transform=ax4.transAxes, fontsize=11,\n",
    "             verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle=\"round,pad=0.4\", facecolor=bg_color, alpha=0.8))\n",
    "    \n",
    "    ax4.set_title('Model Performance Summary', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return r2, rmse, mae, accuracy_5pct, correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6a0cfd-868b-4461-93b9-9743c24f5a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply analysis to enhanced model\n",
    "r2_enh, rmse_enh, mae_enh, accuracy_enh, correlation_enh = plot_essential_analysis(\n",
    "    history_enhanced, model_enhanced, X_val_enhanced, y_val_enhanced, ssh_mean, ssh_std\n",
    ")\n",
    "\n",
    "# Use simple slice comparison with enhanced training data\n",
    "simple_slice_comparison(model_enhanced, ssh_train_enhanced[:4], ssh_mean, ssh_std)\n",
    "\n",
    "# Print final summary\n",
    "print(\"=\"*60)\n",
    "print(\"                    ENHANCED MODEL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training Data Utilization: {total_slices_used}/{len(ssh_norm.t)} ({total_slices_used/len(ssh_norm.t)*100:.1f}%)\")\n",
    "print(f\"Training slices: {len(ssh_train_enhanced)}\")\n",
    "print(f\"Validation slices: {len(ssh_val_enhanced)}\")\n",
    "print(f\"RÂ² Score:           {r2_enh:.4f}\")\n",
    "print(f\"RMSE:              {rmse_enh:.4f} m\")\n",
    "print(f\"MAE:               {mae_enh:.4f} m\")\n",
    "print(f\"Accuracy (Â±5%):    {accuracy_enh:.2f}%\")\n",
    "print(f\"Correlation:       {correlation_enh:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9579709d-2f74-4681-9f39-e9d24dddf88b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
